{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2b5b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe27842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/Users/manu/Desktop/SUP/Projet 2/AI_model_urban_mobility/data/df_final_15min_NoNan_20250505.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d4201",
   "metadata": {},
   "source": [
    "# PREPROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1114b740",
   "metadata": {},
   "source": [
    "## Gestion des données temporelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55617827",
   "metadata": {},
   "source": [
    "Création de profils temporels : des profils regroupant des créneaux horaires similaires selon le jour, l’heure, les vacances et les jours fériés pour chaque tronçon. Ça permet de capturer les habitudes de trafic à différents moments de la semaine.\n",
    "\n",
    "En ajoutant ces profils au modèle, on aide la prédiction à mieux comprendre comment le trafic change selon le temps et le lieu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8b3d6",
   "metadata": {},
   "source": [
    "### Calcul des moyennes horaires par tronçon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433e630",
   "metadata": {},
   "source": [
    "Création d'une variable quarter_hour qui identifie chaque créneau de 15 minutes dans la journée, afin de capturer plus finement la variation temporelle du trafic dans le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946242ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quarter_hour'] = df['hour'] * 4 + df['minute'] // 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf6ba6",
   "metadata": {},
   "source": [
    "### Touver le nombre de cluster ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9287d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k(df_t, max_k=10):\n",
    "    X = df_t[['weekday', 'quarter_hour', 'is_vacances', 'is_ferie']].copy()\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    best_score = -1\n",
    "    best_k = 2\n",
    "\n",
    "    for k in range(2, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        score = silhouette_score(X_scaled, labels)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec327777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recherche de k optimal:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recherche de k optimal: 100%|██████████| 200/200 [03:27<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre optimal de clusters trouvé sur 200 tronçons : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Application sur un echantillon de 200 tronçons aléatoire\n",
    "troncons_sample = np.random.choice(df['troncon_enc'].unique(), size=200, replace=False)\n",
    "k_list = []\n",
    "\n",
    "for troncon in tqdm(troncons_sample, desc=\"Recherche de k optimal\"):\n",
    "    df_t = df[df['troncon_enc'] == troncon]\n",
    "    if len(df_t) >= 10: \n",
    "        k_opt = find_best_k(df_t, max_k=10)\n",
    "        k_list.append(k_opt)\n",
    "\n",
    "# Trouver la valeur la plus fréquente\n",
    "mode_k = Counter(k_list).most_common(1)[0][0]\n",
    "print(f\"Nombre optimal de clusters trouvé sur 200 tronçons : {mode_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4f80f",
   "metadata": {},
   "source": [
    "### Aggrégation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94eaec",
   "metadata": {},
   "source": [
    "Réduit la granularité temporelle et avoir des profils plus stables et moins bruités, ce qui facilite le clustering et donne des clusters plus représentatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce096fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = df.groupby(['troncon_enc', 'weekday', 'quarter_hour', 'is_vacances', 'is_ferie']).agg({\n",
    "    'code_couleur': 'mean'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b3a65",
   "metadata": {},
   "source": [
    "### Application du clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c66f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_time_clusters(df_agg, n_clusters):\n",
    "    all_clusters = []\n",
    "\n",
    "    for troncon in tqdm(df_agg['troncon_enc'].unique(), desc=\"Clustering final par tronçon\"):\n",
    "        df_t = df_agg[df_agg['troncon_enc'] == troncon].copy()\n",
    "        X = df_t[['weekday', 'quarter_hour', 'is_vacances', 'is_ferie']]\n",
    "        X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "        df_t['time_cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "        all_clusters.append(df_t)\n",
    "\n",
    "    return pd.concat(all_clusters, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a51b4c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering final par tronçon: 100%|██████████| 749/749 [00:09<00:00, 78.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Application sur les données agrégées\n",
    "df_clusters = compute_final_time_clusters(agg, n_clusters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "206f9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sur les colonnes clés temporelles et tronçon\n",
    "df_final = df.merge(\n",
    "    df_clusters[['troncon_enc', 'weekday', 'quarter_hour', 'is_vacances', 'is_ferie', 'time_cluster']],\n",
    "    on=['troncon_enc', 'weekday', 'quarter_hour', 'is_vacances', 'is_ferie'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6d12f",
   "metadata": {},
   "source": [
    "### Vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "918b9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880739\n",
      "1880739\n"
     ]
    }
   ],
   "source": [
    "print(len(df_final))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2dacd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>troncon_enc</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [troncon_enc, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compter le nombre de lignes par tronçon\n",
    "count_per_troncon = df_final.groupby('troncon_enc').size().reset_index(name='count')\n",
    "count_per_troncon.loc[count_per_troncon['count'] != 2511]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ddd90",
   "metadata": {},
   "source": [
    "## Gestion des données météo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76b38044",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_vars = ['temperature_2m', 'visibility', 'precipitation', 'wind_speed_10m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe6d0839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_2m    0\n",
      "visibility        0\n",
      "precipitation     0\n",
      "wind_speed_10m    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vérification valeur manquante\n",
    "print(df_final[meteo_vars].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de244b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs négatives dans precipitation : 0\n",
      "Valeurs négatives dans wind_speed_10m : 0\n",
      "Valeurs négatives dans visibility : 0\n"
     ]
    }
   ],
   "source": [
    "# Vérification valeur negatives ou impossibles\n",
    "for var in ['precipitation', 'wind_speed_10m', 'visibility']:\n",
    "    print(f\"Valeurs négatives dans {var} : {(df_final[var] < 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3abea853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_2m min : 0.3305, max : 18.130499\n",
      "visibility min : 2160.0, max : 24140.0\n",
      "precipitation min : 0.0, max : 5.1\n",
      "wind_speed_10m min : 0.8049845, max : 33.127823\n"
     ]
    }
   ],
   "source": [
    "# Vérification valeur extreme\n",
    "for var in meteo_vars:\n",
    "    print(f\"{var} min : {df_final[var].min()}, max : {df_final[var].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9f43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature_2m nombre de valeurs uniques : 270\n",
      "visibility nombre de valeurs uniques : 60\n",
      "precipitation nombre de valeurs uniques : 26\n",
      "wind_speed_10m nombre de valeurs uniques : 446\n"
     ]
    }
   ],
   "source": [
    "# Vérification de la variabilité (ce n'est pas toujours la mm valeur qui revient)\n",
    "for var in meteo_vars:\n",
    "    print(f\"{var} nombre de valeurs uniques : {df_final[var].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a52193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precipitation = 0 dans 1728692 lignes soit 91.92%\n"
     ]
    }
   ],
   "source": [
    "# Vérification de la présennce de pluie\n",
    "for var in ['precipitation']:\n",
    "    zeros = (df_final[var] == 0).sum()\n",
    "    total = len(df_final)\n",
    "    print(f\"{var} = 0 dans {zeros} lignes soit {zeros/total:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af737fec",
   "metadata": {},
   "source": [
    "Conclusion : Les variables météo ne contiennent aucune valeur manquante, ce qui est un bon point pour la qualité des données. Les valeurs extrêmes observées sont dans des plages réalistes pour la région considérée (ex. température entre 0,33°C et 18,13°C, précipitations jusqu’à 5,1 mm sur un quart d’heure).\n",
    "\n",
    "La précipitation est à zéro dans environ 92 % des cas, ce qui est logique car il ne pleut pas tout le temps, mais cela signifie que cette variable peut avoir un impact limité dans certains cas.\n",
    "\n",
    "Dans l’ensemble, les données météo semblent propres et exploitables directement dans le modèle sans nettoyage particulier, mais il faudra surveiller l’impact des variables avec peu de variabilité comme la précipitation souvent nulle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ea134",
   "metadata": {},
   "source": [
    "## Vérification des corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84646e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables les plus corrélées avec code_couleur :\n",
      "taux_occupation      0.819590\n",
      "temps_de_parcours    0.282691\n",
      "vitesse              0.269315\n",
      "debit                0.212483\n",
      "quarter_hour         0.044439\n",
      "hour                 0.044235\n",
      "temperature_2m       0.039239\n",
      "weekday              0.032252\n",
      "wind_speed_10m       0.030965\n",
      "is_vacances          0.030756\n",
      "troncon_enc          0.026609\n",
      "code_no2             0.025660\n",
      "time_cluster         0.008434\n",
      "longueur             0.008411\n",
      "minute               0.005995\n",
      "code_pm25            0.004685\n",
      "code_qual            0.004033\n",
      "precipitation        0.002983\n",
      "code_pm10            0.002673\n",
      "visibility           0.000523\n",
      "is_ferie                  NaN\n",
      "code_zone                 NaN\n",
      "code_so2                  NaN\n",
      "code_o3                   NaN\n",
      "x_wgs84                   NaN\n",
      "y_wgs84                   NaN\n",
      "x_reg                     NaN\n",
      "y_reg                     NaN\n",
      "epsg_reg                  NaN\n",
      "geo_point_2d_lon          NaN\n",
      "geo_point_2d_lat          NaN\n",
      "Name: code_couleur, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculer la corrélation de Pearson entre toutes les variables numériques et 'code_couleur'\n",
    "corr_matrix = df_final.select_dtypes('number').corr()\n",
    "\n",
    "# Extraire la corrélation avec 'code_couleur' et trier par valeur absolue décroissante\n",
    "corr_with_target = corr_matrix['code_couleur'].drop('code_couleur').abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Variables les plus corrélées avec code_couleur :\")\n",
    "print(corr_with_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ce951",
   "metadata": {},
   "source": [
    "Les variables les plus corrélées au trafic sont nos 4 variables temporelles. Mise de côté, on s'aperçoit que quarter_hour est la variable suivante la plus corrélée, bien plus que notre variable de clustering time_cluster. Deux modèles seront testés : 1 - avec le clustering 2 - sans le clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final.to_parquet('/Users/manu/Desktop/SUP/Projet 2/AI_model_urban_mobility/data/data_ready_ml.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbfb62e",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b60dc1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne F1-score macro stratifié sur 5 folds (train+val) : 0.2798\n",
      "F1-score macro sur test : 0.2808\n"
     ]
    }
   ],
   "source": [
    "X = df_final[['temperature_2m', 'visibility', 'precipitation', 'wind_speed_10m', 'has_event_near_troncon', 'troncon_enc', 'time_cluster']]\n",
    "y = df_final['etat_du_trafic']\n",
    "# Calcul des poids inverses des classes (pour gérer le déséquilibre)\n",
    "counts = Counter(y)\n",
    "total = len(y)\n",
    "weights = {cls: total/count for cls, count in counts.items()}\n",
    "sample_weights = y.map(weights).astype(float)\n",
    "\n",
    "# Train-test split stratifié\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "w_train_val = y_train_val.map(weights).astype(float)\n",
    "\n",
    "# Paramètres XGBoost\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # classification multi-classes\n",
    "    'num_class': 4,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = []\n",
    "\n",
    "# Boucle de validation croisée avec encodage des labels\n",
    "for train_idx, val_idx in kf.split(X_train_val, y_train_val):\n",
    "    X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "    y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "    w_train = w_train_val.iloc[train_idx]\n",
    "\n",
    "    # Encodage des labels\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_val_enc = le.transform(y_val)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train_enc, weight=w_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val_enc)\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100,\n",
    "                      evals=[(dval, 'validation')],\n",
    "                      early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    y_pred = model.predict(dval)\n",
    "    f1 = f1_score(y_val_enc, y_pred, average='macro')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f'Moyenne F1-score macro stratifié sur 5 folds (train+val) : {np.mean(f1_scores):.4f}')\n",
    "\n",
    "# Entraînement final sur tout le train+val avec encodage complet\n",
    "le_full = LabelEncoder()\n",
    "y_train_val_enc = le_full.fit_transform(y_train_val)\n",
    "y_test_enc = le_full.transform(y_test)\n",
    "\n",
    "dtrain_full = xgb.DMatrix(X_train_val, label=y_train_val_enc, weight=w_train_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_enc)\n",
    "\n",
    "final_model = xgb.train(params, dtrain_full, num_boost_round=100)\n",
    "\n",
    "# Prédiction et évaluation finale\n",
    "y_test_pred = final_model.predict(dtest)\n",
    "f1_test = f1_score(y_test_enc, y_test_pred, average='macro')\n",
    "print(f'F1-score macro sur test : {f1_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc54c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne F1-score macro stratifié sur 5 folds (train+val) : 0.2881\n",
      "F1-score macro sur test : 0.2908\n"
     ]
    }
   ],
   "source": [
    "X = df_final[['temperature_2m', 'visibility', 'precipitation', 'wind_speed_10m', 'has_event_near_troncon', 'troncon_enc', 'quarter_hour', 'weekday', 'is_vacances']]\n",
    "y = df_final['etat_du_trafic']\n",
    "# Calcul des poids inverses des classes\n",
    "counts = Counter(y)\n",
    "total = len(y)\n",
    "weights = {cls: total/count for cls, count in counts.items()}\n",
    "sample_weights = y.map(weights).astype(float)\n",
    "\n",
    "# Train-test split stratifié\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "w_train_val = y_train_val.map(weights).astype(float)\n",
    "\n",
    "# Paramètres XGBoost\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # classification multi-classes\n",
    "    'num_class': 4,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = []\n",
    "\n",
    "# Boucle de validation croisée avec encodage des labels\n",
    "for train_idx, val_idx in kf.split(X_train_val, y_train_val):\n",
    "    X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "    y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "    w_train = w_train_val.iloc[train_idx]\n",
    "\n",
    "    # Encodage des labels (doit être consécutif 0..num_class-1)\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_val_enc = le.transform(y_val)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train_enc, weight=w_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val_enc)\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100,\n",
    "                      evals=[(dval, 'validation')],\n",
    "                      early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    y_pred = model.predict(dval)\n",
    "    f1 = f1_score(y_val_enc, y_pred, average='macro')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f'Moyenne F1-score macro stratifié sur 5 folds (train+val) : {np.mean(f1_scores):.4f}')\n",
    "\n",
    "# Entraînement final sur tout le train+val avec encodage complet\n",
    "le_full = LabelEncoder()\n",
    "y_train_val_enc = le_full.fit_transform(y_train_val)\n",
    "y_test_enc = le_full.transform(y_test)\n",
    "\n",
    "dtrain_full = xgb.DMatrix(X_train_val, label=y_train_val_enc, weight=w_train_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_enc)\n",
    "\n",
    "final_model = xgb.train(params, dtrain_full, num_boost_round=100)\n",
    "\n",
    "# Prédiction et évaluation finale\n",
    "y_test_pred = final_model.predict(dtest)\n",
    "f1_test = f1_score(y_test_enc, y_test_pred, average='macro')\n",
    "print(f'F1-score macro sur test : {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd91e39",
   "metadata": {},
   "source": [
    "# Amélioration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f289624",
   "metadata": {},
   "source": [
    "### Ne pas équilibrer les classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6bfa10f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne F1-score macro stratifié sur 5 folds (train+val) : 0.2556\n",
      "F1-score macro sur test : 0.2533\n"
     ]
    }
   ],
   "source": [
    "# Données et cibles\n",
    "X = df_final[['temperature_2m', 'visibility', 'precipitation', 'wind_speed_10m',\n",
    "              'has_event_near_troncon', 'troncon_enc', 'quarter_hour', 'weekday', 'is_vacances']]\n",
    "y = df_final['etat_du_trafic']\n",
    "\n",
    "# Train-test split stratifié (sans pondération)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Paramètres XGBoost\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 4,  # Adapter si vous avez 4 classes\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = []\n",
    "\n",
    "# Validation croisée\n",
    "for train_idx, val_idx in kf.split(X_train_val, y_train_val):\n",
    "    X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "    y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "\n",
    "    # Encodage des classes\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_val_enc = le.transform(y_val)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train_enc)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val_enc)\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100,\n",
    "                      evals=[(dval, 'validation')],\n",
    "                      early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    y_pred = model.predict(dval)\n",
    "    f1 = f1_score(y_val_enc, y_pred, average='macro')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f'Moyenne F1-score macro stratifié sur 5 folds (train+val) : {np.mean(f1_scores):.4f}')\n",
    "\n",
    "# Entraînement final\n",
    "le_full = LabelEncoder()\n",
    "y_train_val_enc = le_full.fit_transform(y_train_val)\n",
    "y_test_enc = le_full.transform(y_test)\n",
    "\n",
    "dtrain_full = xgb.DMatrix(X_train_val, label=y_train_val_enc)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_enc)\n",
    "\n",
    "final_model = xgb.train(params, dtrain_full, num_boost_round=100)\n",
    "\n",
    "# Évaluation finale\n",
    "y_test_pred = final_model.predict(dtest)\n",
    "f1_test = f1_score(y_test_enc, y_test_pred, average='macro')\n",
    "print(f'F1-score macro sur test : {f1_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d6e4c",
   "metadata": {},
   "source": [
    "### Créer une nouvelle variable combinée week_day/quarter_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "996d80aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne F1-score macro stratifié sur 5 folds (train+val) : 0.2847\n",
      "F1-score macro sur test : 0.2869\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Création de la nouvelle feature combinée\n",
    "df_final['weekday_quarter'] = df_final['weekday'].astype(str) + '_' + df_final['quarter_hour'].astype(str)\n",
    "\n",
    "# Encodage de la variable combinée\n",
    "le_comb = LabelEncoder()\n",
    "df_final['weekday_quarter_enc'] = le_comb.fit_transform(df_final['weekday_quarter'])\n",
    "\n",
    "# Définition des features et de la cible\n",
    "X = df_final[['temperature_2m', 'visibility', 'precipitation', 'wind_speed_10m',\n",
    "              'has_event_near_troncon', 'troncon_enc', 'is_vacances', 'weekday_quarter_enc']]\n",
    "y = df_final['etat_du_trafic']\n",
    "\n",
    "# Calcul des poids inverses des classes pour gérer le déséquilibre\n",
    "counts = Counter(y)\n",
    "total = len(y)\n",
    "weights = {cls: total/count for cls, count in counts.items()}\n",
    "\n",
    "# Map des poids sur les labels (float)\n",
    "sample_weights = y.map(weights).astype(float)\n",
    "\n",
    "# Train-test split stratifié\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "w_train_val = y_train_val.map(weights).astype(float)\n",
    "\n",
    "# Paramètres XGBoost\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # classification multi-classes\n",
    "    'num_class': y.nunique(),\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = []\n",
    "\n",
    "# Boucle de validation croisée avec encodage des labels et poids\n",
    "for train_idx, val_idx in kf.split(X_train_val, y_train_val):\n",
    "    X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "    y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "    w_train = w_train_val.iloc[train_idx]\n",
    "\n",
    "    # Encodage des labels\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_val_enc = le.transform(y_val)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train_enc, weight=w_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val_enc)\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100,\n",
    "                      evals=[(dval, 'validation')],\n",
    "                      early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    y_pred = model.predict(dval)\n",
    "    f1 = f1_score(y_val_enc, y_pred, average='macro')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f'Moyenne F1-score macro stratifié sur 5 folds (train+val) : {np.mean(f1_scores):.4f}')\n",
    "\n",
    "# Entraînement final avec poids\n",
    "le_full = LabelEncoder()\n",
    "y_train_val_enc = le_full.fit_transform(y_train_val)\n",
    "y_test_enc = le_full.transform(y_test)\n",
    "w_train_val = y_train_val.map(weights).astype(float)\n",
    "\n",
    "dtrain_full = xgb.DMatrix(X_train_val, label=y_train_val_enc, weight=w_train_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_enc)\n",
    "\n",
    "final_model = xgb.train(params, dtrain_full, num_boost_round=100)\n",
    "\n",
    "# Évaluation finale\n",
    "y_test_pred = final_model.predict(dtest)\n",
    "f1_test = f1_score(y_test_enc, y_test_pred, average='macro')\n",
    "print(f'F1-score macro sur test : {f1_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997236b1",
   "metadata": {},
   "source": [
    "### Ne prendre que les variables temporelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64fac74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne F1-score macro stratifié sur 5 folds (train+val) : 0.2126\n",
      "F1-score macro sur test : 0.2115\n"
     ]
    }
   ],
   "source": [
    "# Sélection des variables temporelles uniquement\n",
    "X = df_final[['quarter_hour', 'weekday', 'is_vacances']]\n",
    "y = df_final['etat_du_trafic']\n",
    "\n",
    "# Calcul des poids inverses des classes pour gérer le déséquilibre\n",
    "counts = Counter(y)\n",
    "total = len(y)\n",
    "weights = {cls: total/count for cls, count in counts.items()}\n",
    "\n",
    "# Map des poids sur les labels (float)\n",
    "sample_weights = y.map(weights).astype(float)\n",
    "\n",
    "# Split stratifié train/test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "w_train_val = y_train_val.map(weights).astype(float)\n",
    "\n",
    "# Paramètres XGBoost\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': y.nunique(),\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "f1_scores = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train_val, y_train_val):\n",
    "    X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "    y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "    w_train = w_train_val.iloc[train_idx]\n",
    "\n",
    "    # Encodage des labels (0 à num_class-1)\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_val_enc = le.transform(y_val)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train_enc, weight=w_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val_enc)\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100,\n",
    "                      evals=[(dval, 'validation')],\n",
    "                      early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    y_pred = model.predict(dval)\n",
    "    f1 = f1_score(y_val_enc, y_pred, average='macro')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f'Moyenne F1-score macro stratifié sur 5 folds (train+val) : {np.mean(f1_scores):.4f}')\n",
    "\n",
    "# Entraînement final sur tout le train+val avec poids\n",
    "le_full = LabelEncoder()\n",
    "y_train_val_enc = le_full.fit_transform(y_train_val)\n",
    "y_test_enc = le_full.transform(y_test)\n",
    "w_train_val = y_train_val.map(weights).astype(float)\n",
    "\n",
    "dtrain_full = xgb.DMatrix(X_train_val, label=y_train_val_enc, weight=w_train_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_enc)\n",
    "\n",
    "final_model = xgb.train(params, dtrain_full, num_boost_round=100)\n",
    "\n",
    "# Prédiction finale et évaluation\n",
    "y_test_pred = final_model.predict(dtest)\n",
    "f1_test = f1_score(y_test_enc, y_test_pred, average='macro')\n",
    "print(f'F1-score macro sur test : {f1_test:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
